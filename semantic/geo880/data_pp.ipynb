{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'geo880.txt'\n",
    "PP_DATA_PATH = 'pp_data.json'\n",
    "VOCAB_PATH = 'vocab.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "en_puns = punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data by lines\n",
    "with open(DATA_PATH, 'r') as f:\n",
    "    raw_data_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size = len(raw_data_list)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess entire pair\n",
    "for i in range(len(raw_data_list)):\n",
    "    raw_data_list[i] = raw_data_list[i].lower()\n",
    "    raw_data_list[i] = raw_data_list[i].replace('hamsphire', 'hampshire')\n",
    "    raw_data_list[i] = raw_data_list[i].replace('mississsippi', 'mississippi')\n",
    "    raw_data_list[i] = raw_data_list[i].replace('cites', 'cities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"parse([how,many,people,live,in,the,biggest,city,in,new,york,state,?], answer(a,(population(b,a),largest(b,(city(b),loc(b,c),const(c,stateid('new york')),state(c)))))).\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(raw_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into natural language list and formula language list\n",
    "sample_list, label_list = [], []\n",
    "\n",
    "for pair in raw_data_list:\n",
    "    split_pair = pair.split(', ')\n",
    "    sample_list.append(split_pair[0])\n",
    "    label_list.append(split_pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse([what,are,the,major,rivers,in,ohio,?]\n",
      "answer(a,(major(a),river(a),loc(a,b),const(b,stateid(ohio))))).\n"
     ]
    }
   ],
   "source": [
    "index = random.sample(list(range(len(sample_list))),1)[0]\n",
    "print(sample_list[index])\n",
    "print(label_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepreocess natural language list\n",
    "nl_list = []\n",
    "for i in range(data_size):\n",
    "    sample = sample_list[i]\n",
    "    for p in ['parse'] + list(en_puns):\n",
    "        sample = sample.replace(p, ' ')\n",
    "    sample = ' '.join([token for token in sample.split(' ') if len(token) > 0])\n",
    "    nl_list.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'which is the longest river in usa'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(nl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepreocess formula language list\n",
    "fl_list = []\n",
    "valid_pun = ['\\+', ')', '(', ',']\n",
    "invalid_pun = ['answer(a,', ')).', \"'\"]\n",
    "for i in range(data_size):\n",
    "    label = label_list[i]\n",
    "    for p in invalid_pun:\n",
    "        label = label.replace(p, '')\n",
    "    for p in valid_pun:\n",
    "        label = label.replace(p, ' ' + p + ' ')\n",
    "    label = ' '.join([token for token in label.split(' ') if len(token) > 0])\n",
    "    fl_list.append(label)\n",
    "\n",
    "# for label in label_list:\n",
    "#     label = label.replace(\"('\", '(')\n",
    "#     label = label.replace(\"')\", ')')\n",
    "#     label = label.replace(\"',\", ',')\n",
    "#     label = label[:-1]\n",
    "#     for p in pre_pun:\n",
    "#         label = label.replace(p, ' ' + p)\n",
    "#     for p in post_pun:\n",
    "#         label = label.replace(p, p + ' ')\n",
    "#     fl_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smallest ( b , ( state ( a ) , density ( a , b ) ) )'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(fl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is capital of iowa\n",
      "( capital ( a ) , loc ( a , b ) , const ( b , stateid ( iowa ) ) )\n"
     ]
    }
   ],
   "source": [
    "index = random.sample(list(range(len(nl_list))),1)[0]\n",
    "print(nl_list[index])\n",
    "print(fl_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nl & fl tokenization\n",
    "tk_nl_list, tk_fl_list = [], []\n",
    "\n",
    "for i in range(len(nl_list)):\n",
    "    tk_nl_list.append([nl for nl in nl_list[i].split(' ') if len(nl)>0])\n",
    "    tk_fl_list.append([fl for fl in fl_list[i].split(' ') if len(fl)>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'many', 'rivers', 'are', 'in', 'the', 'state', 'with', 'the', 'highest', 'point']\n",
      "['count', '(', 'b', ',', '(', 'river', '(', 'b', ')', ',', 'loc', '(', 'b', ',', 'c', ')', ',', 'state', '(', 'c', ')', ',', 'loc', '(', 'd', ',', 'c', ')', ',', 'highest', '(', 'd', ',', 'place', '(', 'd', ')', ')', ')', ',', 'a', ')']\n"
     ]
    }
   ],
   "source": [
    "index = random.sample(list(range(len(tk_nl_list))),1)[0]\n",
    "print(tk_nl_list[index])\n",
    "print(tk_fl_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tokens\n",
    "nl_c = Counter()\n",
    "for tk_nl in tk_nl_list:\n",
    "    nl_c.update(tk_nl)\n",
    "\n",
    "nl_token_freq_dict = dict(nl_c)\n",
    "\n",
    "fl_c = Counter()\n",
    "for tk_fl in tk_fl_list:\n",
    "    fl_c.update(tk_fl)\n",
    "\n",
    "fl_token_freq_dict = dict(fl_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n",
      "[('the', 923), ('what', 563), ('is', 418), ('in', 347), ('state', 258), ('states', 256), ('of', 226), ('how', 153), ('which', 131), ('population', 129), ('are', 125), ('river', 118), ('many', 118), ('through', 105), ('largest', 92), ('border', 92), ('rivers', 90), ('point', 87), ('highest', 86), ('cities', 83), ('texas', 82), ('capital', 75), ('has', 74), ('city', 74), ('that', 69), ('with', 67), ('major', 61), ('most', 59), ('smallest', 51), ('run', 47), ('mississippi', 44), ('usa', 42), ('does', 41), ('people', 41), ('have', 40), ('longest', 38), ('density', 36), ('borders', 36), ('lowest', 36), ('area', 36), ('us', 35), ('new', 34), ('colorado', 32), ('live', 31), ('runs', 31), ('where', 26), ('biggest', 23), ('california', 21), ('alaska', 19), ('united', 19), ('austin', 18), ('bordering', 18), ('there', 17), ('me', 16), ('york', 16), ('missouri', 16), ('named', 16), ('elevation', 15), ('all', 14), ('a', 14), ('high', 13), ('long', 13), ('ohio', 13), ('shortest', 13), ('iowa', 13), ('montana', 12), ('populations', 12), ('give', 11), ('name', 11), ('mexico', 11), ('florida', 11), ('length', 11), ('hawaii', 10), ('washington', 10), ('flow', 10), ('alabama', 9), ('pennsylvania', 9), ('not', 9), ('wyoming', 8), ('least', 8), ('maine', 8), ('mountain', 8), ('populous', 8), ('number', 7), ('utah', 7), ('big', 7), ('dakota', 7), ('mount', 7), ('rhode', 7), ('island', 7), ('boulder', 7), ('arizona', 7), ('south', 7), ('springfield', 7), ('michigan', 7), ('illinois', 7), ('traverses', 7), ('virginia', 6), ('points', 6), ('arkansas', 6), ('oregon', 6), ('than', 6), ('lakes', 6), ('delaware', 6), ('capitals', 6), ('citizens', 6), ('kansas', 6), ('san', 6), ('do', 6), ('flows', 6), ('massachusetts', 5), ('mckinley', 5), ('louisiana', 5), ('nebraska', 5), ('georgia', 5), ('by', 5), ('located', 5), ('wisconsin', 5), ('kentucky', 5), ('size', 5), ('running', 5), ('tell', 4), ('north', 4), ('nevada', 4), ('hampshire', 4), ('idaho', 4), ('square', 4), ('next', 4), ('to', 4), ('boston', 4), ('tennessee', 4), ('higher', 4), ('no', 4), ('indiana', 4), ('greatest', 4), ('maryland', 4), ('average', 4), ('dallas', 4), ('contains', 4), ('it', 4), ('surrounding', 3), ('you', 3), ('peak', 3), ('america', 3), ('large', 3), ('rio', 3), ('grande', 3), ('cross', 3), ('houston', 3), ('minnesota', 3), ('one', 3), ('other', 3), ('and', 3), ('rochester', 3), ('50', 3), ('carolina', 3), ('combined', 3), ('seattle', 3), ('height', 3), ('des', 3), ('moines', 3), ('populated', 3), ('atlanta', 3), ('portland', 3), ('total', 3), ('urban', 3), ('can', 2), ('passes', 2), ('inhabitants', 2), ('kalamazoo', 2), ('francisco', 2), ('dc', 2), ('called', 2), ('albany', 2), ('red', 2), ('kilometers', 2), ('at', 2), ('on', 2), ('whose', 2), ('towns', 2), ('tall', 2), ('sacramento', 2), ('antonio', 2), ('show', 2), ('oklahoma', 2), ('vermont', 2), ('neighboring', 2), ('for', 2), ('each', 2), ('per', 2), ('km', 2), ('jersey', 2), ('valley', 2), ('meters', 2), ('country', 2), ('denver', 2), ('tallest', 2), ('mountains', 2), ('excluding', 2), ('s', 2), ('st', 2), ('whitney', 2), ('chattahoochee', 2), ('could', 1), ('count', 1), ('elevations', 1), ('lower', 1), ('guadalupe', 1), ('miles', 1), ('platte', 1), ('montgomery', 1), ('chicago', 1), ('detroit', 1), ('minneapolis', 1), ('riverside', 1), ('spokane', 1), ('lived', 1), ('reside', 1), ('stay', 1), ('residents', 1), ('found', 1), ('traverse', 1), ('longer', 1), ('or', 1), ('much', 1), ('exist', 1), ('list', 1), ('washed', 1), ('names', 1), ('densities', 1), ('potomac', 1), ('about', 1), ('adjacent', 1), ('american', 1), ('continental', 1), ('durham', 1), ('death', 1), ('sea', 1), ('level', 1), ('traversed', 1), ('flowing', 1), ('maximum', 1), ('dense', 1), ('ga', 1), ('erie', 1), ('tempe', 1), ('tucson', 1), ('over', 1), ('though', 1), ('west', 1), ('those', 1), ('salem', 1), ('flint', 1), ('columbus', 1), ('miami', 1), ('pittsburgh', 1), ('dover', 1), ('contain', 1), ('go', 1), ('plano', 1), ('salt', 1), ('lake', 1), ('them', 1), ('neighbor', 1), ('surround', 1), ('whats', 1), ('baton', 1), ('rouge', 1), ('fort', 1), ('wayne', 1), ('indianapolis', 1), ('orleans', 1), ('diego', 1), ('jose', 1), ('scotts', 1), ('spot', 1), ('goes', 1), ('fewest', 1), ('adjoin', 1), ('pass', 1), ('lie', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(len(nl_c))\n",
    "print(nl_c.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "[('(', 5029), (')', 5029), (',', 4488), ('b', 2159), ('a', 1715), ('const', 679), ('c', 614), ('loc', 496), ('state', 489), ('stateid', 404), ('river', 203), ('population', 167), ('next_to', 159), ('city', 151), ('largest', 144), ('traverse', 120), ('place', 103), ('cityid', 99), ('countryid', 96), ('usa', 96), ('d', 86), ('_', 85), ('capital', 81), ('texas', 79), ('highest', 79), ('count', 79), ('riverid', 71), ('major', 61), ('smallest', 61), ('mississippi', 44), ('density', 42), ('most', 42), ('longest', 40), ('area', 36), ('new', 34), ('colorado', 32), ('lowest', 32), ('len', 24), ('california', 21), ('alaska', 19), ('elevation', 18), ('austin', 18), ('york', 16), ('missouri', 15), ('\\\\+', 15), ('size', 14), ('ohio', 13), ('shortest', 13), ('iowa', 13), ('montana', 12), ('mountain', 12), ('mexico', 11), ('florida', 11), ('hawaii', 10), ('alabama', 9), ('placeid', 9), ('e', 8), ('pennsylvania', 8), ('wyoming', 8), ('washington', 8), ('lake', 7), ('utah', 7), ('mount', 7), ('rhode', 7), ('island', 7), ('boulder', 7), ('springfield', 7), ('michigan', 7), ('illinois', 7), ('maine', 7), ('virginia', 6), ('arkansas', 6), ('oregon', 6), ('dakota', 6), ('delaware', 6), ('arizona', 6), ('kansas', 6), ('san', 6), ('south', 6), ('mckinley', 5), ('louisiana', 5), ('nebraska', 5), ('georgia', 5), ('f', 5), ('sum', 5), ('wisconsin', 5), ('kentucky', 5), ('high_point', 4), ('massachusetts', 4), ('north', 4), ('nevada', 4), ('hampshire', 4), ('idaho', 4), ('boston', 4), ('tennessee', 4), ('higher', 4), ('indiana', 4), ('maryland', 4), ('dallas', 4), ('rio', 3), ('grande', 3), ('tx', 3), ('houston', 3), ('rochester', 3), ('carolina', 3), ('seattle', 3), ('des', 3), ('moines', 3), ('atlanta', 3), ('portland', 3), ('fewest', 3), ('low_point', 2), ('kalamazoo', 2), ('francisco', 2), ('wa', 2), ('dc', 2), ('albany', 2), ('red', 2), ('g', 2), ('sacramento', 2), ('antonio', 2), ('oklahoma', 2), ('vermont', 2), ('jersey', 2), ('valley', 2), ('minnesota', 2), ('denver', 2), ('whitney', 2), ('chattahoochee', 2), ('lower', 1), ('guadalupe', 1), ('peak', 1), ('platte', 1), ('montgomery', 1), ('chicago', 1), ('detroit', 1), ('minneapolis', 1), ('mn', 1), ('riverside', 1), ('spokane', 1), ('longer', 1), ('potomac', 1), ('durham', 1), ('death', 1), ('0', 1), ('ma', 1), ('erie', 1), ('pa', 1), ('me', 1), ('mo', 1), ('sd', 1), ('tempe', 1), ('az', 1), ('tucson', 1), ('west', 1), ('salem', 1), ('flint', 1), ('columbus', 1), ('miami', 1), ('pittsburgh', 1), ('dover', 1), ('plano', 1), ('salt', 1), ('baton', 1), ('rouge', 1), ('fort', 1), ('wayne', 1), ('indianapolis', 1), ('orleans', 1), ('diego', 1), ('jose', 1), ('scotts', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(len(fl_c))\n",
    "print(fl_c.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "['could', 'count', 'elevations', 'lower', 'guadalupe', 'miles', 'platte', 'montgomery', 'chicago', 'detroit', 'minneapolis', 'riverside', 'spokane', 'lived', 'reside', 'stay', 'residents', 'found', 'traverse', 'longer', 'or', 'much', 'exist', 'list', 'washed', 'names', 'densities', 'potomac', 'about', 'adjacent', 'american', 'continental', 'durham', 'death', 'sea', 'level', 'traversed', 'flowing', 'maximum', 'dense', 'ga', 'erie', 'tempe', 'tucson', 'over', 'though', 'west', 'those', 'salem', 'flint', 'columbus', 'miami', 'pittsburgh', 'dover', 'contain', 'go', 'plano', 'salt', 'lake', 'them', 'neighbor', 'surround', 'whats', 'baton', 'rouge', 'fort', 'wayne', 'indianapolis', 'orleans', 'diego', 'jose', 'scotts', 'spot', 'goes', 'fewest', 'adjoin', 'pass', 'lie']\n"
     ]
    }
   ],
   "source": [
    "# rare word\n",
    "rare_token_list = [key for key in nl_token_freq_dict if nl_token_freq_dict[key] < 2]\n",
    "print(len(rare_token_list))\n",
    "print(rare_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    }
   ],
   "source": [
    "# generate natural language vocabulary index dictionary\n",
    "nl_vocab_dict = dict()\n",
    "nl_vocab_dict['<s>'] = 0\n",
    "nl_vocab_dict['</s>'] = 1\n",
    "nl_vocab_dict['<pad>'] = 2\n",
    "nl_vocab_dict['<unk>'] = 3\n",
    "i = len(nl_vocab_dict)\n",
    "for token in nl_token_freq_dict:\n",
    "        nl_vocab_dict[token] = i\n",
    "        i += 1\n",
    "print(len(nl_vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "# generate formula language vocabulary index dictionary\n",
    "fl_vocab_dict = dict()\n",
    "fl_vocab_dict['<s>'] = 0\n",
    "fl_vocab_dict['</s>'] = 1\n",
    "fl_vocab_dict['<pad>'] = 2\n",
    "i = len(fl_vocab_dict)\n",
    "for token in fl_token_freq_dict:\n",
    "    fl_vocab_dict[token] = i\n",
    "    i += 1\n",
    "print(len(fl_vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace unknown word with <unk>\n",
    "# for i in range(len(tk_nl_list)):\n",
    "#     for j in range(len(tk_nl_list[i])):\n",
    "#         if tk_nl_list[i][j] not in all_vocab_dict.keys():\n",
    "#             tk_nl_list[i][j] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder inputs\n",
    "encoder_inputs = []\n",
    "for nl in tk_nl_list:\n",
    "    encoder_inputs.append([nl_vocab_dict[token] for token in nl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder inputs\n",
    "decoder_inputs = []\n",
    "for fl in tk_fl_list:\n",
    "    decoder_inputs.append([fl_vocab_dict[token] for token in fl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder targets\n",
    "decoder_targets = []\n",
    "for fl in tk_fl_list:\n",
    "    decoder_targets.append([fl_vocab_dict[token] for token in fl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = np.array(encoder_inputs)\n",
    "decoder_inputs = np.array(decoder_inputs)\n",
    "decoder_targets = np.array(decoder_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = dict()\n",
    "test_dict = dict()\n",
    "\n",
    "index = np.random.permutation(880)\n",
    "index_list = np.split(index, [680, 880])\n",
    "\n",
    "train_dict['encoder_inputs'] = encoder_inputs[index_list[0]].tolist()\n",
    "train_dict['decoder_inputs'] = decoder_inputs[index_list[0]].tolist()\n",
    "train_dict['decoder_targets'] = decoder_targets[index_list[0]].tolist()\n",
    "\n",
    "test_dict['encoder_inputs'] = encoder_inputs[index_list[1]].tolist()\n",
    "test_dict['decoder_inputs'] = decoder_inputs[index_list[1]].tolist()\n",
    "test_dict['decoder_targets'] = decoder_targets[index_list[1]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output as json\n",
    "pp_data = dict()\n",
    "pp_data['train_dict'] = train_dict\n",
    "pp_data['test_dict'] = test_dict\n",
    "\n",
    "vocab_dict = dict()\n",
    "vocab_dict['nl_vocab_dict'] = nl_vocab_dict\n",
    "vocab_dict['fl_vocab_dict'] = fl_vocab_dict\n",
    "\n",
    "with open(PP_DATA_PATH, 'w') as f:\n",
    "    json.dump(pp_data, f, ensure_ascii=False)\n",
    "\n",
    "with open(VOCAB_PATH, 'w') as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
